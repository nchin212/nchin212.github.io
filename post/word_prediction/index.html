<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Word Prediction | Nicholas Chin Wei Lun</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.75.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://nchin212.github.io/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Word Prediction" />
<meta property="og:description" content="Developing a Shiny application for word prediction" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nchin212.github.io/post/word_prediction/" />
<meta property="article:published_time" content="2020-11-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-02T00:00:00+00:00" />
<meta itemprop="name" content="Word Prediction">
<meta itemprop="description" content="Developing a Shiny application for word prediction">
<meta itemprop="datePublished" content="2020-11-02T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-11-02T00:00:00+00:00" />
<meta itemprop="wordCount" content="1507">



<meta itemprop="keywords" content="R,NaturalLanguageProcessing," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word Prediction"/>
<meta name="twitter:description" content="Developing a Shiny application for word prediction"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://nchin212.github.io/images/word_prediction/app.png');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://nchin212.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Nicholas Chin Wei Lun
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://nchin212.github.io/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://nchin212.github.io/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://nchin212.github.io/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      








<a href="https://github.com/nchin212" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Word Prediction</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Developing a Shiny application for word prediction
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://nchin212.github.io/post/word_prediction/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://nchin212.github.io/post/word_prediction/&amp;text=Word%20Prediction" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://nchin212.github.io/post/word_prediction/&amp;title=Word%20Prediction" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Word Prediction</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-11-02T00:00:00Z">November 2, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>Around the world, people are spending an increasing amount of time on
their mobile devices for email, social networking, banking and a whole
range of other activities. But typing on mobile devices can be a serious
pain. The goal of this project is to learn and build predictive text
models in R.</p>
<p>This project was done as part of the Johns Hopkins University Data Science
Capstone Project. In this project, the stupid backoff algorithm will be used
to build a predictive text model. The model will predict the next word based on a
certain score and it will output several possibilities based on the
number of suggestions the user wants. A Shiny application will be
created with this model. When the user types in some words, the model
will give some predictions for the next word and a wordcloud of
predictions will be displayed on screen.</p>
<p>Relevant links:</p>
<p><strong>Shiny Application :</strong>
<!-- raw HTML omitted --><a href="https://nchin212.shinyapps.io/word_prediction_app/">https://nchin212.shinyapps.io/word_prediction_app/</a><!-- raw HTML omitted --></p>
<p><strong>Pitch Presentation :</strong>
<!-- raw HTML omitted --><a href="https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/pitch_presentation/pitch_presentation.html">https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/pitch_presentation/pitch_presentation.html</a><!-- raw HTML omitted --></p>
<p><strong>Github Repository :</strong>
<!-- raw HTML omitted --><a href="https://github.com/nchin212/Coursera_Data_Science_Capstone_Project">https://github.com/nchin212/Coursera_Data_Science_Capstone_Project</a><!-- raw HTML omitted --></p>
<h2 id="data">Data</h2>
<p>The data is downloaded from Coursera and can be obtained
<a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">here</a>.
The data is from a corpus called HC Corpora. The zip file contains 4
folders, which are “en_US”, “de_DE”, “ru_RU” and “fi_FI”. Only
“en_US” contains English text so it was utilised for this project. It
contains 3 text files: blogs, news and twitter.</p>
<h2 id="stupid-backoff-model">Stupid Backoff Model</h2>
<p>Before we dive into the data, it is essential to understand the model we
will be using for prediction. Stupid backoff computes a score rather
than a probability for ngrams on very large datasets. The score is
calculated as follows:</p>
<figure class="fl">
    <img src="https://nchin212.github.io/images/word_prediction/stupid_backoff1.png"/> 
</figure>

<p>where alpha is set to 0.4 and the recursion ends with the unigrams
calculated as follows:</p>
<figure class="fl">
    <img src="https://nchin212.github.io/images/word_prediction/stupid_backoff2.png"/> 
</figure>

<p>Let’s take a look at an example.</p>
<p>Suppose our input text ends with “a great” and we are only working with
1grams, 2grams and 3grams.</p>
<p><strong>Step 1 - Search for the 2gram input in the 3grams dataset :</strong> We
first search in the 3grams dataset for the 3grams that start with the
2grams,“a great”. Suppose 1 of those 3 grams is “a great day”. The score
for “day” is calculated by taking the count of “a great day” in the
3grams dataset divided by the count of “a great” in the 2grams dataset.</p>
<p><strong>Step 2 - Backoff to search for 1 gram input in the 2grams dataset :</strong>
We next search in the 2grams dataset for the 2grams that start with the
1gram, “great”. Suppose 1 of those 2 grams is “great week”. The score
for “week” is calculated by taking the count of “great week” in the
2grams dataset divided by the count of “great” in the 1grams dataset. To
account for the backoff, alpha = 0.4 is multiplied to the result.</p>
<p><strong>Step 3 - Give the most common words in the 1grams dataset :</strong> The
scores for each unigram are calculated by taking their count divided by
the total number of unigrams. To account for the backoff, 0.4**2 is
multiplied to the result.</p>
<p>The model will output the respective words and their scores and the word
with the highest score is most likely to be the next word.</p>
<h2 id="data-cleaning">Data Cleaning</h2>
<p>The datasets are first combined and we notice that there are over 4
million lines altogether. After several trials, 1% of the dataset
appears to be a suitable sample. The sample is then split into training
and validation sets. The training set is transformed into a corpus and
tokenized. Numbers, symbols, punctuation, separators and social media
tags are removed as they create unnecessary noise in the data. Stemming
and removing stopwords are not done as it would affect the prediction
accuracy. Only 1grams, 2grams and 3grams dataframes are created due to
computing limitations.</p>
<p>The dataframes contain the following columns:</p>
<ul>
<li><strong>prevword</strong> - the previous word/words of the ngram (note that 1gram
does not have this column)</li>
<li><strong>nextword</strong> - the last word of the ngram, which is the predicted
next word</li>
<li><strong>score</strong> - the scores computed using stupid backoff</li>
</ul>
<p>The stupid backoff scores for each ngram have been precomputed using the
above algorithm. This will help to save computation time and the
prediction model will be able to predict words faster.</p>
<p>The full data cleaning process can be seen in the following link:</p>
<p><strong>Data Cleaning :</strong>
<!-- raw HTML omitted --><a href="https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/data_cleaning/data_cleaning.html">https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/data_cleaning/data_cleaning.html</a><!-- raw HTML omitted --></p>
<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p>Some exploratory data analysis is also done to check the number of lines
and number of characters. Wordclouds are used to plot the most frequent
1grams, 2grams and 3grams.</p>
<p>The full exploratory data analysis process can be seen in the following
link:</p>
<p><strong>Exploratory Data Analysis :</strong>
<!-- raw HTML omitted --><a href="https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/exploratory_data_analysis/exploratory_data_analysis.html">https://nchin212.github.io/Coursera_Data_Science_Capstone_Project/exploratory_data_analysis/exploratory_data_analysis.html</a><!-- raw HTML omitted --></p>
<h2 id="modelling">Modelling</h2>
<p>The model we will be using is stupid backoff. The stupid backoff scores
have already been computed for the 1grams, 2grams and 3grams dataframes
so now all we have to do is to implement the algorithm itself. The
procedure is as follows:</p>
<p><strong>Step 1 - Take the last 2 words of the user input :</strong> We have to also
account for the cases where the user keys in only 1 word or no words at
all.</p>
<p><strong>Step 2 - Implement the stupid backoff algorithm as seen above :</strong> We
have to retrieve the scores and account for the backoff (alpha = 0.4)
accordingly. If the user keys in no input, the most common words in the
1grams dataset will be predicted as the next word.</p>
<p><strong>Step 3 - Collate the scores and arrange them from highest to lowest :</strong> The word with the highest score is predicted to be the next most
likely word.</p>
<p>Load in functions from other file.</p>
<pre><code>source('../word_prediction_app/functions.R')
source('../data_cleaning/data_cleaning.R')
</code></pre>
<p>The following functions will be used:</p>
<ul>
<li>
<p><strong>fun.input</strong> - Takes in the user input, tokenizes it and retrieves
the last 2 words of the input text</p>
</li>
<li>
<p><strong>fun.predict</strong> - Retrieves the possible next words and stupid
backoff scores according to the algorithm seen above</p>
</li>
</ul>
<p>The functions can be seen in the following link:</p>
<p><strong>Functions :</strong>
<!-- raw HTML omitted --><a href="https://github.com/nchin212/Coursera_Data_Science_Capstone_Project/blob/gh-pages/word_prediction_app/functions.R">https://github.com/nchin212/Coursera_Data_Science_Capstone_Project/blob/gh-pages/word_prediction_app/functions.R</a><!-- raw HTML omitted --></p>
<p>The user inputs the text as follows:</p>
<pre><code>input_text &lt;- 'a great'
</code></pre>
<p>The inputs will be tokenized and the last 2 words will be stored as
separate strings. If the user enters only 1 word, input1 will be an
empty string. If the user enter no words, input1 and input2 will be
empty strings.</p>
<pre><code>input1 = fun.input(input_text)[1,]
input2 = fun.input(input_text)[2,]
input1

## [1] &quot;a&quot;

input2

## [1] &quot;great&quot;
</code></pre>
<p>We run the predict function and retrieve 5 predictions, i.e. the top 5
words with the highest scores.</p>
<pre><code>number_pred &lt;- 5
fun.predict(input1, input2)

## # A tibble: 5 x 2
##   nextword  score
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 time     0.107
## 2 day      0.0849
## 3 weekend  0.0440
## 4 deal     0.0346
## 5 way      0.0346
</code></pre>
<p>As seen above, “time” has the highest score of 0.107, followed by “day”
and “weekend”. The predicted words appear to be fairly accurate.</p>
<h2 id="accuracy">Accuracy</h2>
<p>To determine its accuracy, we run the model on the validation set
prepared during the data cleaning process.</p>
<pre><code>source('../validation/validation.R')
</code></pre>
<p>The full validation process can be seen in the following link:</p>
<p><strong>Validation :</strong>
<!-- raw HTML omitted --><a href="https://github.com/nchin212/Coursera_Data_Science_Capstone_Project/blob/gh-pages/validation/validation.R">https://github.com/nchin212/Coursera_Data_Science_Capstone_Project/blob/gh-pages/validation/validation.R</a><!-- raw HTML omitted --></p>
<p>The accuracy table is printed as follows:</p>
<pre><code>accuracy_table

##           Pred5 Pred3 Pred1
## Previous2  0.68  0.61  0.50
## Previous1  0.60  0.50  0.37
</code></pre>
<p>Here, accuracy means the percentage of cases where the correct word is
predicted within the defined number of predicted words. The table shows
that the model is able to predict the next word relatively accurately
with an accuracy of 0.68 when there are 5 predictions and using 2
previous words. The model does not do well when there is only 1
prediction and using 1 previous word.</p>
<p>Perhaps other models such has Katz’s backoff model and BERT could be
implemented to give a better result. Smoothing techniques such as
Kneeser-Ney smoothing may also improve the accuracy.</p>
<h2 id="challenges">Challenges</h2>
<p>This section describes the challenges faced during this project.</p>
<p><strong>Challenge 1 - NLP is not easy :</strong> This was my first project on
natural language processing and I had barely any knowledge of it. This
course had a hands off approach so I had to do much research
independently. It took several weeks but it was an interesting
experience. Natural language processing is a huge field and little did I
know that there were this many NLP libraries developed in R.</p>
<p><strong>Challenge 2 - Memory :</strong> Due to the dataset containing millions of
lines, it was difficult to determine a suitable sample size as I had to
balance between sample size and prediction accuracy of the model.
Certain functions in R used up more memory as compared to others and
slow computation time was an issue several times. The stupid backoff
score computation and retrieval of the scores also got more
computationally intensive for 4grams and 5grams so I had to leave them
out. Thus, this project has taught me to consider computer memory when
working on large datasets.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>Overall, this was a very rewarding project as I had gained much
knowledge on the aspects of word prediction and I was very satisfied
with the final application designed. If you have made it this far, thank
you for taking the time to read through my project! Many thanks to
Coursera for providing me this opportunity to try out a new field.</p>
<p><a href="https://github.com/nchin212/Coursera_Data_Science_Capstone_Project">Link to Github repository</a></p>
<ul class="pa0">
  
   <li class="list">
     <a href="https://nchin212.github.io/tags/R" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">R</a>
   </li>
  
   <li class="list">
     <a href="https://nchin212.github.io/tags/NaturalLanguageProcessing" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">NaturalLanguageProcessing</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://nchin212.github.io" >
    &copy;  Nicholas Chin Wei Lun 2021 
  </a>
    <div>








<a href="https://github.com/nchin212" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://nchin212.github.io/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
